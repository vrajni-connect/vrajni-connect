{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a94425d-0b3b-4044-8288-ec5514c4f205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/sh\n",
    "\n",
    "split_num=$1\n",
    "moc_string=MOC-`date \"+%m-%Y-%d\"`\n",
    "#moc_string=temp_MOC-09-2023\n",
    "\n",
    "export instance1=$2\n",
    "\n",
    "export project=ProjectId\n",
    "export subnet=projects/ProjectId/regions/Zone/subnetworks/subnetID \n",
    "export zone1=$3\n",
    "export image=VM-Image\n",
    "export scopes=https://www.googleapis.com/auth/cloud-platform\n",
    "\n",
    "export source_code=gs://source-code-bucket/sarima_learn\n",
    "export split_path=gs://input-data-bucket/folder1/project-input/supply-chain/sarima\n",
    "export gcs_workloc=$split_path/$moc_string\n",
    "\n",
    "gsutil cp $source_code/sarima_learn_rsbp_splits.sh .\n",
    "\n",
    "sed -i -e 's/\\r$//' sarima_learn_rsbp_splits.sh\n",
    "chmod a+x sarima_learn_rsbp_splits.sh\n",
    "\n",
    "bash -x sarima_learn_rsbp_splits.sh $split_num\n",
    "\n",
    "export no_of_inst=`ls sarima_learn_splits/* | wc -l`\n",
    "final_vm=$((no_of_inst-$split_num))\n",
    "echo $final_vm\n",
    "\n",
    "##########################################\n",
    "\n",
    "echo \"Modeling started for sarima-learn::$(date +%d_%m_%Y_%H_%M_%S)\"\n",
    "\n",
    "ls sarima_learn_splits/* | cut -c21-$NF  >split_filenames.txt\n",
    "\n",
    "gcloud compute instances bulk create --name-pattern=sarima-learn-#### --no-address --count=$no_of_inst --machine-type=$instance1 --subnet=$subnet --zone=$zone1 --image=$image --scopes=$scopes --labels=model=sarima-learn --format=json > ec2run\n",
    "\n",
    "wait $! \n",
    "\n",
    "mkdir -p output error\n",
    "rm -f hosts.txt instanceids.txt \n",
    "rm output/* error/*\n",
    "\n",
    "gsutil -m cp $source_code/ec2InstanceParse.py .\n",
    "\n",
    "sudo chmod a+x ec2InstanceParse.py\n",
    "\n",
    "python3 ec2InstanceParse.py `pwd`/ ec2run $no_of_inst\n",
    "\n",
    "LINECOUNTER=1\n",
    "\n",
    "while read host;\n",
    "do\n",
    "\n",
    "FILE1INPUT=\"$(sed -n \"${LINECOUNTER}p\" split_filenames.txt)\"\n",
    "\n",
    "#gcloud config set account $service_account\n",
    "nohup gcloud compute ssh $host --project=$project --zone=$zone1 --quiet --command \"\n",
    "rm -rf sarima; mkdir sarima; chmod 777 sarima; cd sarima\n",
    "export filename=$FILE1INPUT\n",
    "export gcs_path=gs://input-data-bucket/folder1/project-input/supply-chain/sarima\n",
    "export gcs_workloc=$gcs_workloc\n",
    "export zone=$zone1\n",
    "export source_code=$source_code\n",
    "mkdir output; chmod 777 output; mkdir error; chmod 777 error\n",
    "gsutil -m cp $source_code/sarima_learn_add_data.sh . >cpcheck\n",
    "sed -i -e 's/\\r$//' sarima_learn_add_data.sh\n",
    "chmod a+x sarima_learn_add_data.sh\n",
    "nohup bash -x sarima_learn_add_data.sh >output/$host.out 2>error/$host.err & \n",
    "exit \" >output/$host.out 2>error/$host.err &\n",
    "LINECOUNTER=$((LINECOUNTER+1))\n",
    "done < hosts.txt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "JumpToRemoteVms",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
